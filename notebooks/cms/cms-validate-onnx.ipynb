{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e7ac6c-997d-4ecf-9a6f-b96f593713dc",
   "metadata": {},
   "source": [
    "This notebook is responsible for exporting the MLPF trained model from pytorch to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a5a5d-8f56-45a8-b649-7933a777f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnxscript\n",
    "# !pip install onnxconverter-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52b47c-e6c7-4399-9686-e240da62fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numba\n",
    "import awkward\n",
    "import vector\n",
    "import fastjet\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep\n",
    "\n",
    "import boost_histogram as bh\n",
    "import mplhep\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "import onnxscript\n",
    "import onnx\n",
    "import onnxruntime as rt\n",
    "from onnxconverter_common import float16\n",
    "from onnxscript.function_libs.torch_lib.tensor_typing import TFloat\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "import mlpf\n",
    "from mlpf.model.mlpf import MLPF\n",
    "from mlpf.model.utils import unpack_predictions, unpack_target\n",
    "from mlpf.jet_utils import match_jets, to_p4_sph\n",
    "from mlpf.plotting.plot_utils import cms_label, sample_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac816c-5c34-4c13-8598-67f72d564fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mplhep.style.use(\"CMS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2bd59-4edb-4603-99e5-1d1d94e89f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which onnxruntime we are using. For CUDA, we must use onnxruntime-gpu (not onnxruntime)\n",
    "rt.__path__, rt.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8413bda-1e12-44fd-968e-590806f3da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrib op: https://github.com/microsoft/onnxruntime/blob/main/docs/ContribOperators.md#commicrosoftmultiheadattention\n",
    "# CMSSW ONNXRuntime version: https://github.com/cms-sw/cmsdist/blob/REL/CMSSW_14_1_0_pre3/el9_amd64_gcc12/onnxruntime.spec\n",
    "# ONNXRuntime compatiblity table: https://onnxruntime.ai/docs/reference/compatibility.html\n",
    "\n",
    "#with pytorch 2.5.0, we should use at least opset 20 (previous opsets did not work)\n",
    "from onnxscript import opset20 as op\n",
    "opset_version = 20\n",
    "\n",
    "custom_opset = onnxscript.values.Opset(domain=\"onnx-script\", version=1)\n",
    "msft_op = onnxscript.values.Opset(\"com.microsoft\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d0792-ec61-4168-802c-7e914f1d449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfds datasets are here:\n",
    "data_dir = \"/scratch/persistent/joosep/tensorflow_datasets/\"\n",
    "dataset = \"cms_pf_ttbar_nopu\"\n",
    "\n",
    "#model checkpoints are here:\n",
    "outdir = \"../../experiments/pyg-cms_20241212_101648_120237/\"\n",
    "\n",
    "#Load model weights from existing training\n",
    "model_state = torch.load(\n",
    "    outdir + \"/checkpoints/checkpoint-05-3.498507.pth\", map_location=torch.device(\"cpu\")\n",
    ")\n",
    "with open(f\"{outdir}/model_kwargs.pkl\", \"rb\") as f:\n",
    "    model_kwargs = pkl.load(f)\n",
    "\n",
    "#this is needed to configure com.microsoft.MultiHeadAttention\n",
    "NUM_HEADS = model_kwargs[\"num_heads\"]\n",
    "\n",
    "#set this to cuda if you are running the notebook on a GPU, otherwise use cpu\n",
    "torch_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac122980-f83f-4d7e-a6c3-6d75843d5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the base model from our codebase, initialize with trained weights\n",
    "model = MLPF(**model_kwargs)\n",
    "model.eval()\n",
    "model.load_state_dict(model_state[\"model_state_dict\"])\n",
    "\n",
    "model = model.to(device=torch_device)\n",
    "\n",
    "#disable attention context manager (disable flash attention)\n",
    "for conv in model.conv_id + model.conv_reg:\n",
    "    conv.enable_ctx_manager = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867be21-685e-45d4-828b-3f9855ca8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are copied from mlpf.py to be able to explicitly edit the model here in the notebook\n",
    "def get_activation(activation):\n",
    "    if activation == \"elu\":\n",
    "        act = nn.ELU\n",
    "    elif activation == \"relu\":\n",
    "        act = nn.ReLU\n",
    "    elif activation == \"relu6\":\n",
    "        act = nn.ReLU6\n",
    "    elif activation == \"leakyrelu\":\n",
    "        act = nn.LeakyReLU\n",
    "    return act\n",
    "\n",
    "def ffn(input_dim, output_dim, width, act, dropout):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, width),\n",
    "        act(),\n",
    "        torch.nn.LayerNorm(width),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(width, output_dim),\n",
    "    )\n",
    "\n",
    "\n",
    "class RegressionOutput(nn.Module):\n",
    "    def __init__(self, mode, embed_dim, width, act, dropout, elemtypes):\n",
    "        super(RegressionOutput, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.elemtypes = elemtypes\n",
    "\n",
    "        # single output\n",
    "        if self.mode == \"direct\" or self.mode == \"additive\" or self.mode == \"multiplicative\":\n",
    "            self.nn = ffn(embed_dim, 1, width, act, dropout)\n",
    "        elif self.mode == \"direct-elemtype\":\n",
    "            self.nn = ffn(embed_dim, len(self.elemtypes), width, act, dropout)\n",
    "        elif self.mode == \"direct-elemtype-split\":\n",
    "            self.nn = nn.ModuleList()\n",
    "            for elem in range(len(self.elemtypes)):\n",
    "                self.nn.append(ffn(embed_dim, 1, width, act, dropout))\n",
    "        # two outputs\n",
    "        elif self.mode == \"linear\":\n",
    "            self.nn = ffn(embed_dim, 2, width, act, dropout)\n",
    "        elif self.mode == \"linear-elemtype\":\n",
    "            self.nn1 = ffn(embed_dim, len(self.elemtypes), width, act, dropout)\n",
    "            self.nn2 = ffn(embed_dim, len(self.elemtypes), width, act, dropout)\n",
    "\n",
    "    def forward(self, elems, x, orig_value):\n",
    "        if self.mode == \"direct\":\n",
    "            nn_out = self.nn(x)\n",
    "            return nn_out\n",
    "        elif self.mode == \"direct-elemtype\":\n",
    "            nn_out = self.nn(x)\n",
    "            elemtype_mask = torch.cat([elems[..., 0:1] == elemtype for elemtype in self.elemtypes], axis=-1)\n",
    "            nn_out = torch.sum(elemtype_mask * nn_out, axis=-1, keepdims=True)\n",
    "            return nn_out\n",
    "        elif self.mode == \"direct-elemtype-split\":\n",
    "            elem_outs = []\n",
    "            for elem in range(len(self.elemtypes)):\n",
    "                elem_outs.append(self.nn[elem](x))\n",
    "            elemtype_mask = torch.cat([elems[..., 0:1] == elemtype for elemtype in self.elemtypes], axis=-1)\n",
    "            elem_outs = torch.cat(elem_outs, axis=-1)\n",
    "            return torch.sum(elem_outs * elemtype_mask, axis=-1, keepdims=True)\n",
    "        elif self.mode == \"additive\":\n",
    "            nn_out = self.nn(x)\n",
    "            return orig_value + nn_out\n",
    "        elif self.mode == \"multiplicative\":\n",
    "            nn_out = self.nn(x)\n",
    "            return orig_value * nn_out\n",
    "        elif self.mode == \"linear\":\n",
    "            nn_out = self.nn(x)\n",
    "            return orig_value * nn_out[..., 0:1] + nn_out[..., 1:2]\n",
    "        elif self.mode == \"linear-elemtype\":\n",
    "            nn_out1 = self.nn1(x)\n",
    "            nn_out2 = self.nn2(x)\n",
    "            elemtype_mask = torch.cat([elems[..., 0:1] == elemtype for elemtype in self.elemtypes], axis=-1)\n",
    "            a = torch.sum(elemtype_mask * nn_out1, axis=-1, keepdims=True)\n",
    "            b = torch.sum(elemtype_mask * nn_out2, axis=-1, keepdims=True)\n",
    "            return orig_value * a + b\n",
    "\n",
    "#an explicit implementation of torch multihead attention to debug the forward pass implementation in detail\n",
    "class SimpleMultiheadAttention(nn.MultiheadAttention):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        bias = True\n",
    "        batch_first = True\n",
    "        super().__init__(embed_dim, num_heads, dropout, bias=bias, batch_first=batch_first, **factory_kwargs)\n",
    "        self.head_dim = int(embed_dim // num_heads)\n",
    "        \n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=bias, **factory_kwargs)\n",
    "        self.export_onnx = False\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        #q, k, v: 3D tensors (batch_size, seq_len, embed_dim), embed_dim = num_heads*head_dim\n",
    "        bs, seq_len, embed_dim = q.size()\n",
    "        head_dim = self.head_dim\n",
    "        num_heads = self.num_heads\n",
    "\n",
    "        #split stacked in_proj_weight, in_proj_bias to q, k, v matrices\n",
    "        wq, wk, wv = torch.split(self.in_proj_weight.data, [self.embed_dim, self.embed_dim, self.embed_dim], dim=0)\n",
    "        bq, bk, bv = torch.split(self.in_proj_bias.data, [self.embed_dim, self.embed_dim, self.embed_dim], dim=0)\n",
    "\n",
    "        q = torch.matmul(q, wq.T) + bq\n",
    "        k = torch.matmul(k, wk.T) + bk\n",
    "        v = torch.matmul(v, wv.T) + bv\n",
    "\n",
    "        #for pytorch internal scaled dot product attention, we need (bs*num_heads, seq_len, head_dim)\n",
    "        if not self.export_onnx:\n",
    "            q = q.reshape(bs, seq_len, num_heads, head_dim).transpose(1,2).reshape(bs*num_heads, seq_len, head_dim)\n",
    "            k = k.reshape(bs, seq_len, num_heads, head_dim).transpose(1,2).reshape(bs*num_heads, seq_len, head_dim)\n",
    "            v = v.reshape(bs, seq_len, num_heads, head_dim).transpose(1,2).reshape(bs*num_heads, seq_len, head_dim)\n",
    "\n",
    "        #this function will have different shape signatures in native pytorch sdpa and in ONNX com.microsoft.MultiHeadAttention\n",
    "        #in pytorch: (bs*num_heads, seq_len, head_dim)\n",
    "        #in ONNX: (bs, seq_len, num_heads*head_dim)\n",
    "        attn_output = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout)\n",
    "\n",
    "        #in case running with pytorch internal scaled dot product attention, reshape back to the original shape\n",
    "        if not self.export_onnx:\n",
    "            attn_output = attn_output.reshape(bs, num_heads, seq_len, head_dim).transpose(1,2).reshape(bs, seq_len, num_heads*head_dim)\n",
    "        \n",
    "        assert list(attn_output.size()) == [bs, seq_len, num_heads * head_dim]\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output, None\n",
    "\n",
    "\n",
    "class SimplePreLnSelfAttentionLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"\",\n",
    "        activation=\"elu\",\n",
    "        embedding_dim=128,\n",
    "        num_heads=2,\n",
    "        width=128,\n",
    "        dropout_mha=0.1,\n",
    "        dropout_ff=0.1,\n",
    "        attention_type=\"efficient\",\n",
    "        learnable_queries=False,\n",
    "        elems_as_queries=False,\n",
    "    ):\n",
    "        super(SimplePreLnSelfAttentionLayer, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "        # set to False to enable manual override for ONNX export\n",
    "        self.enable_ctx_manager = False\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        self.act = get_activation(activation)\n",
    "        # self.mha = torch.nn.MultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha, batch_first=True)\n",
    "        self.mha = SimpleMultiheadAttention(embedding_dim, num_heads, dropout=dropout_mha)\n",
    "        self.norm0 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.seq = torch.nn.Sequential(nn.Linear(embedding_dim, width), self.act(), nn.Linear(width, embedding_dim), self.act())\n",
    "        self.dropout = torch.nn.Dropout(dropout_ff)\n",
    "\n",
    "        # params for torch sdp_kernel\n",
    "        if self.enable_ctx_manager:\n",
    "            self.attn_params = {\n",
    "                \"math\": [SDPBackend.MATH],\n",
    "                \"efficient\": [SDPBackend.EFFICIENT_ATTENTION],\n",
    "                \"flash\": [SDPBackend.FLASH_ATTENTION],\n",
    "            }\n",
    "\n",
    "        self.learnable_queries = learnable_queries\n",
    "        self.elems_as_queries = elems_as_queries\n",
    "        if self.learnable_queries:\n",
    "            self.queries = nn.Parameter(torch.zeros(1, 1, embedding_dim), requires_grad=True)\n",
    "            trunc_normal_(self.queries, std=0.02)\n",
    "\n",
    "        self.save_attention = False\n",
    "        self.outdir = \"\"\n",
    "\n",
    "    def forward(self, x, mask, initial_embedding):\n",
    "        mask_ = mask.unsqueeze(-1)\n",
    "        x = self.norm0(x * mask_)\n",
    "\n",
    "        q = x\n",
    "        if self.learnable_queries:\n",
    "            q = self.queries.expand(*x.shape) * mask_\n",
    "        elif self.elems_as_queries:\n",
    "            q = initial_embedding * mask_\n",
    "\n",
    "        key_padding_mask = None\n",
    "        if self.attention_type == \"math\":\n",
    "            key_padding_mask = ~mask\n",
    "\n",
    "        # default path, for FlashAttn/Math backend\n",
    "        if self.enable_ctx_manager:\n",
    "            with sdpa_kernel(self.attn_params[self.attention_type]):\n",
    "                mha_out = self.mha(q, x, x)[0]\n",
    "\n",
    "        # path for ONNX export\n",
    "        else:\n",
    "            mha_out = self.mha(q, x, x)[0]\n",
    "\n",
    "        mha_out = mha_out * mask_\n",
    "\n",
    "        mha_out = x + mha_out\n",
    "        x = self.norm1(mha_out)\n",
    "        x = mha_out + self.seq(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x * mask_\n",
    "        return x\n",
    "\n",
    "#this is a standalone copy of the MLPF model code\n",
    "#we copy it here so that we can debug everything about the model in a single notebook\n",
    "class SimpleMLPF(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=34,\n",
    "        num_classes=8,\n",
    "        embedding_dim=128,\n",
    "        width=128,\n",
    "        num_convs=2,\n",
    "        dropout_ff=0.0,\n",
    "        activation=\"elu\",\n",
    "        layernorm=True,\n",
    "        conv_type=\"attention\",\n",
    "        input_encoding=\"joint\",\n",
    "        pt_mode=\"linear\",\n",
    "        eta_mode=\"linear\",\n",
    "        sin_phi_mode=\"linear\",\n",
    "        cos_phi_mode=\"linear\",\n",
    "        energy_mode=\"linear\",\n",
    "        # element types which actually exist in the dataset\n",
    "        elemtypes_nonzero=[1, 4, 5, 6, 8, 9, 10, 11],\n",
    "        # should the conv layer outputs be concatted (concat) or take the last (last)\n",
    "        learned_representation_mode=\"last\",\n",
    "        # gnn-lsh specific parameters\n",
    "        bin_size=640,\n",
    "        max_num_bins=200,\n",
    "        distance_dim=128,\n",
    "        num_node_messages=2,\n",
    "        ffn_dist_hidden_dim=128,\n",
    "        ffn_dist_num_layers=2,\n",
    "        # self-attention specific parameters\n",
    "        num_heads=16,\n",
    "        head_dim=16,\n",
    "        attention_type=\"flash\",\n",
    "        dropout_conv_reg_mha=0.0,\n",
    "        dropout_conv_reg_ff=0.0,\n",
    "        dropout_conv_id_mha=0.0,\n",
    "        dropout_conv_id_ff=0.0,\n",
    "        use_pre_layernorm=False,\n",
    "    ):\n",
    "        super(SimpleMLPF, self).__init__()\n",
    "\n",
    "        self.conv_type = conv_type\n",
    "\n",
    "        self.act = get_activation(activation)\n",
    "\n",
    "        self.learned_representation_mode = learned_representation_mode\n",
    "\n",
    "        self.input_encoding = input_encoding\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.num_convs = num_convs\n",
    "\n",
    "        self.bin_size = bin_size\n",
    "        self.elemtypes_nonzero = elemtypes_nonzero\n",
    "\n",
    "        self.use_pre_layernorm = use_pre_layernorm\n",
    "\n",
    "        if self.conv_type == \"attention\":\n",
    "            embedding_dim = num_heads * head_dim\n",
    "            width = num_heads * head_dim\n",
    "\n",
    "        # embedding of the inputs\n",
    "        if self.num_convs != 0:\n",
    "            if self.input_encoding == \"joint\":\n",
    "                self.nn0_id = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "                self.nn0_reg = ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff)\n",
    "            elif self.input_encoding == \"split\":\n",
    "                self.nn0_id = nn.ModuleList()\n",
    "                for ielem in range(len(self.elemtypes_nonzero)):\n",
    "                    self.nn0_id.append(ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff))\n",
    "                self.nn0_reg = nn.ModuleList()\n",
    "                for ielem in range(len(self.elemtypes_nonzero)):\n",
    "                    self.nn0_reg.append(ffn(self.input_dim, embedding_dim, width, self.act, dropout_ff))\n",
    "\n",
    "            if self.conv_type == \"attention\":\n",
    "                self.conv_id = nn.ModuleList()\n",
    "                self.conv_reg = nn.ModuleList()\n",
    "\n",
    "                for i in range(self.num_convs):\n",
    "                    lastlayer = i == self.num_convs - 1\n",
    "                    self.conv_id.append(\n",
    "                        SimplePreLnSelfAttentionLayer(\n",
    "                            name=\"conv_id_{}\".format(i),\n",
    "                            activation=activation,\n",
    "                            embedding_dim=embedding_dim,\n",
    "                            num_heads=num_heads,\n",
    "                            width=width,\n",
    "                            dropout_mha=dropout_conv_id_mha,\n",
    "                            dropout_ff=dropout_conv_id_ff,\n",
    "                            attention_type=attention_type,\n",
    "                            elems_as_queries=lastlayer,\n",
    "                            # learnable_queries=lastlayer,\n",
    "                        )\n",
    "                    )\n",
    "                    self.conv_reg.append(\n",
    "                        SimplePreLnSelfAttentionLayer(\n",
    "                            name=\"conv_reg_{}\".format(i),\n",
    "                            activation=activation,\n",
    "                            embedding_dim=embedding_dim,\n",
    "                            num_heads=num_heads,\n",
    "                            width=width,\n",
    "                            dropout_mha=dropout_conv_reg_mha,\n",
    "                            dropout_ff=dropout_conv_reg_ff,\n",
    "                            attention_type=attention_type,\n",
    "                            elems_as_queries=lastlayer,\n",
    "                            # learnable_queries=lastlayer,\n",
    "                        )\n",
    "                    )\n",
    "            elif self.conv_type == \"gnn_lsh\":\n",
    "                self.conv_id = nn.ModuleList()\n",
    "                self.conv_reg = nn.ModuleList()\n",
    "                for i in range(self.num_convs):\n",
    "                    gnn_conf = {\n",
    "                        \"inout_dim\": embedding_dim,\n",
    "                        \"bin_size\": self.bin_size,\n",
    "                        \"max_num_bins\": max_num_bins,\n",
    "                        \"distance_dim\": distance_dim,\n",
    "                        \"layernorm\": layernorm,\n",
    "                        \"num_node_messages\": num_node_messages,\n",
    "                        \"dropout\": dropout_ff,\n",
    "                        \"ffn_dist_hidden_dim\": ffn_dist_hidden_dim,\n",
    "                        \"ffn_dist_num_layers\": ffn_dist_num_layers,\n",
    "                    }\n",
    "                    self.conv_id.append(CombinedGraphLayer(**gnn_conf))\n",
    "                    self.conv_reg.append(CombinedGraphLayer(**gnn_conf))\n",
    "\n",
    "        if self.learned_representation_mode == \"concat\":\n",
    "            decoding_dim = self.num_convs * embedding_dim\n",
    "        elif self.learned_representation_mode == \"last\":\n",
    "            decoding_dim = embedding_dim\n",
    "\n",
    "        # DNN that acts on the node level to predict the PID\n",
    "        self.nn_binary_particle = ffn(decoding_dim, 2, width, self.act, dropout_ff)\n",
    "        self.nn_pid = ffn(decoding_dim, num_classes, width, self.act, dropout_ff)\n",
    "\n",
    "        # elementwise DNN for node momentum regression\n",
    "        embed_dim = decoding_dim\n",
    "        self.nn_pt = RegressionOutput(pt_mode, embed_dim, width, self.act, dropout_ff, self.elemtypes_nonzero)\n",
    "        self.nn_eta = RegressionOutput(eta_mode, embed_dim, width, self.act, dropout_ff, self.elemtypes_nonzero)\n",
    "        self.nn_sin_phi = RegressionOutput(sin_phi_mode, embed_dim, width, self.act, dropout_ff, self.elemtypes_nonzero)\n",
    "        self.nn_cos_phi = RegressionOutput(cos_phi_mode, embed_dim, width, self.act, dropout_ff, self.elemtypes_nonzero)\n",
    "        self.nn_energy = RegressionOutput(energy_mode, embed_dim, width, self.act, dropout_ff, self.elemtypes_nonzero)\n",
    "\n",
    "        if self.use_pre_layernorm:  # add final norm after last attention block as per https://arxiv.org/abs/2002.04745\n",
    "            self.final_norm_id = torch.nn.LayerNorm(decoding_dim)\n",
    "            self.final_norm_reg = torch.nn.LayerNorm(embed_dim)\n",
    "\n",
    "    # @torch.compile\n",
    "    def forward(self, X_features, mask):\n",
    "        Xfeat_normed = X_features\n",
    "        mask = mask.bool()\n",
    "\n",
    "        embeddings_id, embeddings_reg = [], []\n",
    "        if self.num_convs != 0:\n",
    "            if self.input_encoding == \"joint\":\n",
    "                embedding_id = self.nn0_id(Xfeat_normed)\n",
    "                embedding_reg = self.nn0_reg(Xfeat_normed)\n",
    "            elif self.input_encoding == \"split\":\n",
    "                embedding_id = torch.stack([nn0(Xfeat_normed) for nn0 in self.nn0_id], axis=-1)\n",
    "                elemtype_mask = torch.cat([X_features[..., 0:1] == elemtype for elemtype in self.elemtypes_nonzero], axis=-1)\n",
    "                embedding_id = torch.sum(embedding_id * elemtype_mask.unsqueeze(-2), axis=-1)\n",
    "    \n",
    "                embedding_reg = torch.stack([nn0(Xfeat_normed) for nn0 in self.nn0_reg], axis=-1)\n",
    "                elemtype_mask = torch.cat([X_features[..., 0:1] == elemtype for elemtype in self.elemtypes_nonzero], axis=-1)\n",
    "                embedding_reg = torch.sum(embedding_reg * elemtype_mask.unsqueeze(-2), axis=-1)\n",
    "    \n",
    "                for num, conv in enumerate(self.conv_id):\n",
    "                    conv_input = embedding_id if num == 0 else embeddings_id[-1]\n",
    "                    out_padded = conv(conv_input, mask, embedding_id)\n",
    "                    embeddings_id.append(out_padded)\n",
    "                for num, conv in enumerate(self.conv_reg):\n",
    "                    conv_input = embedding_reg if num == 0 else embeddings_reg[-1]\n",
    "                    out_padded = conv(conv_input, mask, embedding_reg)\n",
    "                    embeddings_reg.append(out_padded)\n",
    "\n",
    "        # id input\n",
    "        if self.learned_representation_mode == \"concat\":\n",
    "            final_embedding_id = torch.cat(embeddings_id, axis=-1)\n",
    "        elif self.learned_representation_mode == \"last\":\n",
    "            final_embedding_id = torch.cat([embeddings_id[-1]], axis=-1)\n",
    "\n",
    "        if self.use_pre_layernorm:\n",
    "            final_embedding_id = self.final_norm_id(final_embedding_id)\n",
    "\n",
    "        preds_binary_particle = self.nn_binary_particle(final_embedding_id)\n",
    "        preds_pid = self.nn_pid(final_embedding_id)\n",
    "\n",
    "        # pred_charge = self.nn_charge(final_embedding_id)\n",
    "\n",
    "        # regression input\n",
    "        if self.learned_representation_mode == \"concat\":\n",
    "            final_embedding_reg = torch.cat(embeddings_reg, axis=-1)\n",
    "        elif self.learned_representation_mode == \"last\":\n",
    "            final_embedding_reg = torch.cat([embeddings_reg[-1]], axis=-1)\n",
    "\n",
    "        # if self.use_pre_layernorm:\n",
    "        final_embedding_reg = self.final_norm_reg(final_embedding_reg)\n",
    "\n",
    "        # The PFElement feature order in X_features defined in fcc/postprocessing.py\n",
    "        preds_pt = self.nn_pt(X_features, final_embedding_reg, X_features[..., 1:2])\n",
    "        preds_eta = self.nn_eta(X_features, final_embedding_reg, X_features[..., 2:3])\n",
    "        preds_sin_phi = self.nn_sin_phi(X_features, final_embedding_reg, X_features[..., 3:4])\n",
    "        preds_cos_phi = self.nn_cos_phi(X_features, final_embedding_reg, X_features[..., 4:5])\n",
    "\n",
    "        # ensure created particle has positive mass^2 by computing energy from pt and adding a positive-only correction\n",
    "        pt_real = torch.exp(preds_pt.detach()) * X_features[..., 1:2]\n",
    "        # sinh is not supported by ONNX so use exp instead of pz_real = pt_real * torch.sinh(preds_eta.detach())\n",
    "        detached_preds_eta = preds_eta.detach()\n",
    "        pz_real = pt_real * (torch.exp(detached_preds_eta) - torch.exp(-detached_preds_eta)) / 2\n",
    "        \n",
    "        e_real = torch.log(torch.sqrt(pt_real**2 + pz_real**2) / X_features[..., 5:6])\n",
    "        # the regular torch indexing of the mask results in changed tensor shapes in the ONNX model\n",
    "        # so we use torch.Tensor.masked_scatter_() instead of e_real[~mask] = 0\n",
    "        e_real.masked_scatter_(~mask.unsqueeze(-1), torch.zeros(size=(0, torch.sum(~mask))).to(device=mask.device))\n",
    "        e_real[torch.isinf(e_real)] = 0\n",
    "        e_real[torch.isnan(e_real)] = 0\n",
    "        preds_energy = e_real + torch.nn.functional.relu(self.nn_energy(X_features, final_embedding_reg, X_features[..., 5:6]))\n",
    "        preds_momentum = torch.cat([preds_pt, preds_eta, preds_sin_phi, preds_cos_phi, preds_energy], axis=-1)\n",
    "        # assert list(preds_momentum.size())[-1] == 5, list(preds_momentum.size())\n",
    "        return preds_binary_particle, preds_pid, preds_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19c4ac-e579-46c2-86d0-6f664f4b8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple = SimpleMLPF(**model_kwargs)\n",
    "model_simple.eval()\n",
    "\n",
    "#disable attention context manager (disable flash attention)\n",
    "for conv in model_simple.conv_id + model_simple.conv_reg:\n",
    "    conv.enable_ctx_manager = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9248e-a729-4171-88a7-3ab06c1268e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple.load_state_dict(model_state[\"model_state_dict\"])\n",
    "\n",
    "#the values in here are not important, it just needs some kind of shapes\n",
    "dummy_features = torch.randn(1, 256, model_kwargs[\"input_dim\"]).float()\n",
    "dummy_mask = torch.randn(1, 256).float()\n",
    "\n",
    "#export the ONNX model with naive (unfused) attention\n",
    "torch.onnx.export(\n",
    "    model_simple,\n",
    "    (dummy_features, dummy_mask),\n",
    "    \"test_fp32_unfused.onnx\",\n",
    "    opset_version=opset_version,\n",
    "    verbose=False,\n",
    "    input_names=[\n",
    "        \"Xfeat_normed\", \"mask\",\n",
    "    ],\n",
    "    output_names=[\"bid\", \"id\", \"momentum\"],\n",
    "    dynamic_axes={\n",
    "        \"Xfeat_normed\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "        \"mask\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "        \"bid\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "        \"id\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "        \"momentum\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecf124-8610-476b-9a08-94a3513d1074",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_simple_fused = copy.deepcopy(model_simple)\n",
    "\n",
    "#configure the model to run in (batch, seq_len, num_heads*head_dim) 3d-mode.\n",
    "for conv in model_simple_fused.conv_id + model_simple_fused.conv_reg:\n",
    "    conv.mha.export_onnx = True\n",
    "\n",
    "#register our custom op that calls out to the fast MultiHeadAttention implementation\n",
    "@onnxscript.script(custom_opset)\n",
    "def SDPA(\n",
    "    query: TFloat,\n",
    "    key: TFloat,\n",
    "    value: TFloat,\n",
    ") -> TFloat:\n",
    "\n",
    "    # Unlike pytorch scaled_dot_product_attention,\n",
    "    # the input here MUST BE (batch, seq_len, num_head*head_dim).\n",
    "    # Also, for the op to be fast on GPU, it needs to run in float16.\n",
    "    query = op.Cast(query, to=onnx.TensorProto.FLOAT16)\n",
    "    key = op.Cast(key, to=onnx.TensorProto.FLOAT16)\n",
    "    value = op.Cast(value, to=onnx.TensorProto.FLOAT16)\n",
    "    output, _, _ = msft_op.MultiHeadAttention(query, key, value, num_heads=NUM_HEADS)\n",
    "    output = op.Cast(output, to=onnx.TensorProto.FLOAT)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# setType API provides shape/type to ONNX shape/type inference\n",
    "# function signature must match pytorch aten::scaled_dot_product_attention from\n",
    "# https://github.com/pytorch/pytorch/blob/16676fd17b10b06e692656bbba8db5e0d6052a20/aten/src/ATen/native/transformers/attention.cpp#L699\n",
    "def custom_scaled_dot_product_attention(\n",
    "    g, query: TFloat, key: TFloat, value: TFloat, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None, enable_gqa=False\n",
    "):\n",
    "    return g.onnxscript_op(SDPA, query, key, value).setType(query.type())\n",
    "\n",
    "\n",
    "#the warning 'MultiHeadAttention' is not a known op in 'com.microsoft' is not actually important\n",
    "print(\"registering custom op for scaled_dot_product_attention\")\n",
    "torch.onnx.register_custom_op_symbolic(\n",
    "    symbolic_name=\"aten::scaled_dot_product_attention\",\n",
    "    symbolic_fn=custom_scaled_dot_product_attention,\n",
    "    opset_version=opset_version,\n",
    ")\n",
    "\n",
    "torch.onnx.export(\n",
    "    model_simple_fused,\n",
    "    (dummy_features, dummy_mask),\n",
    "    \"test_fp32_fused.onnx\",\n",
    "    opset_version=opset_version,\n",
    "    verbose=False,\n",
    "    input_names=[\n",
    "        \"Xfeat_normed\", \"mask\",\n",
    "    ],\n",
    "    output_names=[\"bid\", \"id\", \"momentum\"],\n",
    "    dynamic_axes={\n",
    "        \"Xfeat_normed\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "        \"mask\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "        \"bid\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "        \"id\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "        \"momentum\": {0: \"num_batch\", 1: \"num_elements\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e50424-5a61-4c9c-8f5b-6df132dd1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available ONNX runtime providers:\", rt.get_available_providers())\n",
    "sess_options = rt.SessionOptions()\n",
    "sess_options.intra_op_num_threads = 32  # need to explicitly set this to get rid of onnxruntime error\n",
    "\n",
    "sess_options.log_severity_level = 1\n",
    "sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "\n",
    "execution_provider = \"CPUExecutionProvider\"\n",
    "if torch_device.type == \"cuda\":\n",
    "    execution_provider = \"CUDAExecutionProvider\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf9b61-6e80-4b79-9526-d545294e3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_sess_unfused = rt.InferenceSession(\"test_fp32_unfused.onnx\", sess_options, providers=[execution_provider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264eb8e-bd01-482c-a9c9-de6c7587e58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_sess_fused = rt.InferenceSession(\"test_fp32_fused.onnx\", sess_options, providers=[execution_provider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc3e15-be3f-4dcd-9f31-7f6e1eb82561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffs_vec(pred_reference, pred_test):\n",
    "    diffs = [torch.mean(torch.abs(torch.flatten(pred_reference[i]-pred_test[i]))).item() for i in range(len(pred_test))]\n",
    "    return diffs\n",
    "\n",
    "#cluster particles to jets, return jet pt\n",
    "def particles_to_jets(pred, mask):\n",
    "    jetdef = fastjet.JetDefinition(fastjet.antikt_algorithm, 0.4)\n",
    "    ypred = unpack_predictions(pred)\n",
    "    for k, v in ypred.items():\n",
    "        ypred[k] = v[mask].detach().cpu().contiguous().numpy()\n",
    "    \n",
    "    counts = torch.sum(mask, axis=1).cpu().numpy()\n",
    "    clsid = awkward.unflatten(ypred[\"cls_id\"], counts)\n",
    "    msk = clsid != 0\n",
    "    p4 = awkward.unflatten(ypred[\"p4\"], counts)\n",
    "    \n",
    "    vec = vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"pt\": p4[msk][:, :, 0],\n",
    "                \"eta\": p4[msk][:, :, 1],\n",
    "                \"phi\": p4[msk][:, :, 2],\n",
    "                \"e\": p4[msk][:, :, 3],\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    cluster = fastjet.ClusterSequence(vec.to_xyzt(), jetdef)\n",
    "    jets = cluster.inclusive_jets(min_pt=3)\n",
    "    return jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff46497-e83d-48ec-99e6-140ed7047111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over the dataset, run the 4 different models, save outputs\n",
    "builder = tfds.builder(dataset, data_dir=data_dir)\n",
    "ds = builder.as_data_source(split=\"test\")\n",
    "\n",
    "max_events = 100\n",
    "events_per_batch = 1\n",
    "inds = range(0, max_events, events_per_batch)\n",
    "\n",
    "jets_mlpf = []\n",
    "jets_mlpf_simple = []\n",
    "jets_onnx_unfused = []\n",
    "jets_onnx_fused = []\n",
    "\n",
    "model = model.to(torch_device)\n",
    "model_simple = model_simple.to(torch_device)\n",
    "\n",
    "for ind in tqdm(inds):\n",
    "    ds_elems = [ds[i] for i in range(ind,ind+events_per_batch)]\n",
    "    X_features = [torch.tensor(elem[\"X\"]).to(torch.float32).to(torch_device) for elem in ds_elems]\n",
    "    y_targets = [torch.tensor(elem[\"ytarget\"]).to(torch.float32).to(torch_device) for elem in ds_elems]\n",
    "\n",
    "    #batch the data into [batch_size, num_elems, num_features]\n",
    "    X_features_padded = pad_sequence(X_features, batch_first=True).contiguous()\n",
    "    y_targets_padded = pad_sequence(y_targets, batch_first=True).contiguous()\n",
    "    # print(\"batch\", ind, X_features_padded.shape)\n",
    "    mask = X_features_padded[:, :, 0]!=0\n",
    "    mask_f = mask.float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # print(\"running base pytorch model\")\n",
    "        pred = model(X_features_padded.to(torch_device), mask.to(torch_device))\n",
    "        pred = tuple(pred[x].cpu() for x in range(len(pred)))\n",
    "        # print(\"running simplified pytorch model\")\n",
    "        pred_simple = model_simple(X_features_padded.to(torch_device), mask.to(torch_device))\n",
    "        pred_simple = tuple(pred_simple[x].cpu() for x in range(len(pred_simple)))\n",
    "\n",
    "    j0 = particles_to_jets(pred, mask.cpu())\n",
    "    jets_mlpf.append(j0)\n",
    "    \n",
    "    j1 = particles_to_jets(pred_simple, mask.cpu())\n",
    "    jets_mlpf_simple.append(j1)\n",
    "\n",
    "    #test that the classification and regression outputs are close between the original and simplified pytorch models\n",
    "    # diffs = diffs_vec(pred, pred_simple)\n",
    "    # print(\"diffs: {:.8f} {:.8f}\".format(*diffs))\n",
    "    torch.testing.assert_close(pred[0], pred_simple[0], atol=0.01, rtol=0.01)\n",
    "    torch.testing.assert_close(pred[1], pred_simple[1], atol=0.01, rtol=0.01)\n",
    "\n",
    "    # print(\"running ONNX unfused model\")\n",
    "    pred_onnx_unfused = onnx_sess_unfused.run([\"bid\", \"id\", \"momentum\"], {\"Xfeat_normed\": X_features_padded.cpu().numpy(), \"mask\": mask_f.cpu().numpy()})\n",
    "    pred_onnx_unfused = tuple(torch.tensor(p) for p in pred_onnx_unfused)\n",
    "    j2 = particles_to_jets(pred_onnx_unfused, mask.cpu())\n",
    "    jets_onnx_unfused.append(j2)\n",
    "    # diffs = diffs_vec(pred_simple, pred_onnx_unfused)\n",
    "    # print(\"diffs: {:.8f} {:.8f}\".format(*diffs))\n",
    "    torch.testing.assert_close(pred_simple[0], pred_onnx_unfused[0], atol=0.01, rtol=0.01)\n",
    "    torch.testing.assert_close(pred_simple[1], pred_onnx_unfused[1], atol=0.01, rtol=0.01)\n",
    "    \n",
    "    # print(\"running ONNX fused model\")\n",
    "    pred_onnx_fused = onnx_sess_fused.run([\"bid\", \"id\", \"momentum\"], {\"Xfeat_normed\": X_features_padded.cpu().numpy(), \"mask\": mask_f.cpu().numpy()})\n",
    "    pred_onnx_fused = tuple(torch.tensor(p) for p in pred_onnx_fused)\n",
    "    j3 = particles_to_jets(pred_onnx_fused, mask.cpu())\n",
    "    jets_onnx_fused.append(j3)\n",
    "    # diffs = diffs_vec(pred_onnx_unfused, pred_onnx_fused)\n",
    "    # print(\"diffs: {:.8f} {:.8f}\".format(*diffs))\n",
    "    # this does not exactly close on CUDAExecutionProvider\n",
    "    # torch.testing.assert_close(pred_onnx_unfused[0], pred_onnx_fused[0], atol=0.1, rtol=0.1)\n",
    "    # torch.testing.assert_close(pred_onnx_unfused[1], pred_onnx_fused[1], atol=0.1, rtol=0.1)\n",
    "\n",
    "    # print(\"jets\", awkward.count(j0), awkward.count(j1), awkward.count(j2), awkward.count(j3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1435d5e-efba-44f7-aa29-54481d8490dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_overflow_into_last_bin(all_values):\n",
    "    values = all_values[1:-1]\n",
    "    values[-1] = values[-1] + all_values[-1]\n",
    "    values[0] = values[0] + all_values[0]\n",
    "    return values\n",
    "\n",
    "def to_bh(data, bins, cumulative=False):\n",
    "    h1 = bh.Histogram(bh.axis.Variable(bins))\n",
    "    h1.fill(data)\n",
    "    if cumulative:\n",
    "        h1[:] = np.sum(h1.values()) - np.cumsum(h1)\n",
    "    h1[:] = sum_overflow_into_last_bin(h1.values(flow=True)[:])\n",
    "    return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441021e7-c1da-4459-a7ed-ebf85be4c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There can be cases where different inference modes produce very slightly different numbers of jets due to floating point differences.\n",
    "#Therefore, we match jet pairs based on delta-R, and compare the pT of matched jets.\n",
    "match_inds1, match_inds2 = match_jets(to_p4_sph(awkward.concatenate(jets_mlpf)), to_p4_sph(awkward.concatenate(jets_mlpf_simple)), 0.001)\n",
    "match_inds3, match_inds4 = match_jets(to_p4_sph(awkward.concatenate(jets_mlpf_simple)), to_p4_sph(awkward.concatenate(jets_onnx_unfused)), 0.001)\n",
    "match_inds5, match_inds6 = match_jets(to_p4_sph(awkward.concatenate(jets_onnx_unfused)), to_p4_sph(awkward.concatenate(jets_onnx_fused)), 0.001)\n",
    "match_inds7, match_inds8 = match_jets(to_p4_sph(awkward.concatenate(jets_mlpf)), to_p4_sph(awkward.concatenate(jets_onnx_fused)), 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fcfdd-087d-4e22-9243-9d84f25c169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.logspace(0, 2, 200)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.hist2d(\n",
    "    awkward.to_numpy(awkward.flatten(awkward.concatenate(jets_mlpf)[match_inds1].pt)),\n",
    "    awkward.to_numpy(awkward.flatten(awkward.concatenate(jets_mlpf_simple)[match_inds2].pt)),\n",
    "    bins=b,\n",
    "    norm=mpl.colors.LogNorm(),\n",
    "    cmap=\"Reds\"\n",
    ");\n",
    "plt.xlabel(\"jet $p_{\\mathrm{T,pytorch}}$\")\n",
    "plt.ylabel(\"jet $p_{\\mathrm{T,pytorch,simple}}$\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f501b02f-ca20-41a9-ae94-e6857854e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.logspace(0, 2, 200)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.hist2d(\n",
    "    awkward.to_numpy(awkward.flatten(awkward.concatenate(jets_mlpf_simple)[match_inds3].pt)),\n",
    "    awkward.to_numpy(awkward.flatten(awkward.concatenate(jets_onnx_unfused)[match_inds4].pt)),\n",
    "    bins=b,\n",
    "    norm=mpl.colors.LogNorm(),\n",
    "    cmap=\"Reds\"\n",
    ");\n",
    "plt.xlabel(\"jet $p_{\\mathrm{T,pytorch,simple}}$\")\n",
    "plt.ylabel(\"jet $p_{\\mathrm{T,ONNX}}$\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1d1a4-6f7d-4615-8303-09d5408fbe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.logspace(0, 2, 200)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.hist2d(\n",
    "    awkward.to_numpy(awkward.flatten(awkward.concatenate(jets_onnx_unfused)[match_inds5].pt)),\n",
    "    awkward.to_numpy(awkward.flatten(awkward.concatenate(jets_onnx_fused)[match_inds6].pt)),\n",
    "    bins=b,\n",
    "    norm=mpl.colors.LogNorm(),\n",
    "    cmap=\"Reds\"\n",
    ");\n",
    "plt.xlabel(\"jet $p_{\\mathrm{T,ONNX}}$\")\n",
    "plt.ylabel(\"jet $p_{\\mathrm{T,ONNX,fused}}$\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad8671-f7c0-4dc5-8c9a-576f7fcaa95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.logspace(0, 2, 200)\n",
    "plt.figure(figsize=(12,10))\n",
    "ax = plt.axes()\n",
    "plt.hist2d(\n",
    "    awkward.to_numpy(awkward.flatten(awkward.concatenate(jets_mlpf)[match_inds7].pt)),\n",
    "    awkward.to_numpy(awkward.flatten(awkward.concatenate(jets_onnx_fused)[match_inds8].pt)),\n",
    "    bins=b,\n",
    "    norm=mpl.colors.LogNorm(),\n",
    "    cmap=\"Reds\"\n",
    ");\n",
    "plt.xlabel(\"jet $p_{\\mathrm{T,pytorch}}$\")\n",
    "plt.ylabel(\"jet $p_{\\mathrm{T,ONNX}}$\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.colorbar()\n",
    "cms_label(ax)\n",
    "sample_label(ax, \"cms_pf_ttbar_nopu\")\n",
    "plt.savefig(\"pytorch_onnx_jet_pt_2d.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7718c2a1-e2c6-4097-b168-7ed53d6cdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_inds9, match_inds10 = match_jets(to_p4_sph(awkward.concatenate(jets_mlpf)), to_p4_sph(awkward.concatenate(jets_onnx_fused)), 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a815a1f-b22f-4552-b6d1-4ce23f6c462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward.count(match_inds9)/awkward.count(awkward.concatenate(jets_mlpf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9971b29-2efa-4dba-a5e9-46369a6919a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(0.9,1.1, 500)\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "plt.hist(\n",
    "    awkward.flatten(awkward.concatenate(jets_onnx_fused)[match_inds10].pt/awkward.concatenate(jets_mlpf)[match_inds9].pt),\n",
    "    bins=b, histtype=\"step\");\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"jet $p_{\\mathrm{T,ONNX,fused}}/p_{\\mathrm{T,pytorch}}$\")\n",
    "plt.ylabel(\"Number of matched jets\")\n",
    "cms_label(ax)\n",
    "sample_label(ax, \"cms_pf_ttbar_nopu\")\n",
    "plt.savefig(\"pytorch_onnx_jet_ratio.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026531f-07d7-4451-be73-caf68960a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (a0, a1) = plt.subplots(2, 1, gridspec_kw={\"height_ratios\": [4, 1]}, sharex=True, figsize=(10, 10))\n",
    "\n",
    "plt.sca(a0)\n",
    "\n",
    "b = np.logspace(0,2,101)\n",
    "h0 = to_bh(awkward.flatten(awkward.concatenate(jets_mlpf).pt), bins=b)\n",
    "h1 = to_bh(awkward.flatten(awkward.concatenate(jets_mlpf_simple).pt), bins=b)\n",
    "h2 = to_bh(awkward.flatten(awkward.concatenate(jets_onnx_unfused).pt), bins=b)\n",
    "h3 = to_bh(awkward.flatten(awkward.concatenate(jets_onnx_fused).pt), bins=b)\n",
    "\n",
    "mplhep.histplot(h0, label=\"pytorch\", lw=0.5, yerr=0)\n",
    "# mplhep.histplot(h1, label=\"pytorch, simplified\", lw=0.5, yerr=0)\n",
    "# mplhep.histplot(h2, label=\"ONNX, unfused\", lw=0.5, yerr=0)\n",
    "mplhep.histplot(h3, label=\"ONNX, fused\", lw=0.5, yerr=0)\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"Number of jets\")\n",
    "cms_label(a0)\n",
    "sample_label(a0, \"cms_pf_ttbar_nopu\")\n",
    "\n",
    "plt.sca(a1)\n",
    "mplhep.histplot(h0/h0, label=\"pytorch\", lw=0.5, yerr=0)\n",
    "# mplhep.histplot(h1/h0, label=\"pytorch, simplified\", lw=0.5, yerr=0)\n",
    "# mplhep.histplot(h2/h0, label=\"ONNX, unfused\", lw=0.5, yerr=0)\n",
    "mplhep.histplot(h3/h0, label=\"ONNX, fused\", lw=0.5, yerr=0)\n",
    "plt.ylim(0.5, 1.5)\n",
    "plt.xlim(1, 100)\n",
    "plt.ylabel(\"vs. pytorch\")\n",
    "plt.xlabel(\"jet $p_T$ [GeV]\")\n",
    "plt.savefig(\"pytorch_onnx_jet_pt.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
