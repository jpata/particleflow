{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quickstart notebook allows to test and mess around with the MLPF GNN model in a standalone way. For actual training, we don't use a notebook, please refer to `README.md`.\n",
    "\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/jpata/particleflow/\n",
    "```\n",
    "\n",
    "Run the notebook from `notebooks/delphes-tf-mlpf-quickstart.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2, pickle\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle as pkl\n",
    "import os.path as osp\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path += [\"../mlpf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘tev14_pythia8_ttbar_0_0.pkl.bz2’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget --no-check-certificate -nc https://zenodo.org/record/4452283/files/tev14_pythia8_ttbar_0_0.pkl.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(bz2.BZ2File(\"tev14_pythia8_ttbar_0_0.pkl.bz2\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#100 events in one file\n",
    "len(data[\"X\"]), len(data[\"ygen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad the number of elements to a size that's divisible by the bin size\n",
    "Xs = []\n",
    "ys = []\n",
    "\n",
    "max_size = 50*128\n",
    "for i in range(len(data[\"X\"])):\n",
    "    X = data[\"X\"][i][:max_size, :]\n",
    "    y = data[\"ygen\"][i][:max_size, :]\n",
    "    Xpad = np.pad(X, [(0, max_size - X.shape[0]), (0, 0)])\n",
    "    ypad = np.pad(y, [(0, max_size - y.shape[0]), (0, 0)])\n",
    "    Xpad = Xpad.astype(np.float32)\n",
    "    ypad = ypad.astype(np.float32)\n",
    "    Xs.append(Xpad)\n",
    "    ys.append(ypad)\n",
    "    \n",
    "X = np.stack(Xs)\n",
    "y = np.stack(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the pytorch setup for the input X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a pytorch FCN class for particleflow \n",
    "\n",
    "class MLPF_FCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Showcase an example of an fully connected network pytorch model, with a skip connection, that can be explained by LRP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=12, hidden_dim=2, embedding_dim=2, output_dim=2):\n",
    "        super(MLPF_FCN, self).__init__()\n",
    "\n",
    "        self.act = nn.ReLU\n",
    "\n",
    "        self.nn1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "        )\n",
    "        self.nn2 = nn.Sequential(\n",
    "            nn.Linear(input_dim + embedding_dim, hidden_dim),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        embedding = self.nn1(X)\n",
    "        return self.nn2(torch.cat([X, embedding], axis=1)), _, _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a pytorch GNN class for particleflow \n",
    "\n",
    "import pickle as pkl\n",
    "import os.path as osp\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Union\n",
    "from torch_geometric.typing import OptTensor, PairTensor, PairOptTensor\n",
    "from torch_geometric.data import Data, DataLoader, DataListLoader, Batch\n",
    "\n",
    "try:\n",
    "    from torch_cluster import knn\n",
    "except ImportError:\n",
    "    knn = None\n",
    "from torch_cluster import knn_graph\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MLPF_GNN(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN model based on Gravnet...\n",
    "\n",
    "    Forward pass returns\n",
    "        preds: tensor of predictions containing a concatenated representation of the pids and p4\n",
    "        A: dict() object containing adjacency matrices for each message passing\n",
    "        msg_activations: dict() object containing activations before each message passing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim=12, output_dim_id=6, output_dim_p4=6,\n",
    "                 embedding_dim=2, hidden_dim1=2, hidden_dim2=2,\n",
    "                 num_convs=2, space_dim=4, propagate_dim=2, k=8):\n",
    "        super(MLPF_GNN, self).__init__()\n",
    "\n",
    "        # self.act = nn.ReLU\n",
    "        self.act = nn.ELU\n",
    "\n",
    "        # (1) embedding\n",
    "        self.nn1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim1),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim1),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim1, embedding_dim),\n",
    "        )\n",
    "\n",
    "        self.conv = nn.ModuleList()\n",
    "        for i in range(num_convs):\n",
    "            self.conv.append(GravNetConv_LRP(embedding_dim, embedding_dim, space_dim, propagate_dim, k))\n",
    "\n",
    "        # (3) DNN layer: classifiying pid\n",
    "        self.nn2 = nn.Sequential(\n",
    "            nn.Linear(input_dim + embedding_dim, hidden_dim2),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim2),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim2),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim2, output_dim_id),\n",
    "        )\n",
    "\n",
    "        # (4) DNN layer: regressing p4\n",
    "        self.nn3 = nn.Sequential(\n",
    "            nn.Linear(input_dim + output_dim_id, hidden_dim2),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim2),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim2, hidden_dim2),\n",
    "            self.act(),\n",
    "            nn.Linear(hidden_dim2, output_dim_p4),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        x0 = batch.x\n",
    "\n",
    "        # embed the inputs\n",
    "        embedding = self.nn1(x0)\n",
    "\n",
    "        # preform a series of graph convolutions\n",
    "        A = {}\n",
    "        msg_activations = {}\n",
    "        for num, conv in enumerate(self.conv):\n",
    "            embedding, A[f'conv.{num}'], msg_activations[f'conv.{num}'] = conv(embedding)\n",
    "\n",
    "        # predict the pid's\n",
    "        preds_id = self.nn2(torch.cat([x0, embedding], axis=-1))\n",
    "\n",
    "        # predict the p4's\n",
    "        preds_p4 = self.nn3(torch.cat([x0, preds_id], axis=-1))\n",
    "\n",
    "        return torch.cat([preds_id, preds_p4], axis=-1), A, msg_activations\n",
    "\n",
    "\n",
    "class GravNetConv_LRP(MessagePassing):\n",
    "    \"\"\"\n",
    "    Copied from pytorch_geometric source code, with the following edits\n",
    "      a. retrieve adjacency matrix (we call A), and the activations before the message passing step (we call msg_activations)\n",
    "      b. switched the execution of self.lin_s & self.lin_p so that the message passing step can substitute out of the box self.lin_s for lrp purposes\n",
    "      c. used reduce='sum' instead of reduce='mean' in the message passing\n",
    "      d. removed skip connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 space_dimensions: int, propagate_dimensions: int, k: int,\n",
    "                 num_workers: int = 1, **kwargs):\n",
    "        super().__init__(flow='source_to_target', **kwargs)\n",
    "\n",
    "        if knn is None:\n",
    "            raise ImportError('`GravNetConv` requires `torch-cluster`.')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.k = k\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.lin_p = Linear(in_channels, propagate_dimensions)\n",
    "        self.lin_s = Linear(in_channels, space_dimensions)\n",
    "        self.lin_out = Linear(propagate_dimensions, out_channels)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_s.reset_parameters()\n",
    "        self.lin_p.reset_parameters()\n",
    "        self.lin_out.reset_parameters()\n",
    "\n",
    "    def forward(\n",
    "            self, x: Union[Tensor, PairTensor],\n",
    "            batch: Union[OptTensor, Optional[PairTensor]] = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "        is_bipartite: bool = True\n",
    "        if isinstance(x, Tensor):\n",
    "            x: PairTensor = (x, x)\n",
    "            is_bipartite = False\n",
    "\n",
    "        if x[0].dim() != 2:\n",
    "            raise ValueError(\"Static graphs not supported in 'GravNetConv'\")\n",
    "\n",
    "        b: PairOptTensor = (None, None)\n",
    "        if isinstance(batch, Tensor):\n",
    "            b = (batch, batch)\n",
    "        elif isinstance(batch, tuple):\n",
    "            assert batch is not None\n",
    "            b = (batch[0], batch[1])\n",
    "\n",
    "        # embed the inputs before message passing\n",
    "        msg_activations = self.lin_p(x[0])\n",
    "\n",
    "        # transform to the space dimension to build the graph\n",
    "        s_l: Tensor = self.lin_s(x[0])\n",
    "        s_r: Tensor = self.lin_s(x[1]) if is_bipartite else s_l\n",
    "\n",
    "        edge_index = knn(s_l, s_r, self.k, b[0], b[1]).flip([0])\n",
    "\n",
    "        edge_weight = (s_l[edge_index[0]] - s_r[edge_index[1]]).pow(2).sum(-1)\n",
    "        edge_weight = torch.exp(-10. * edge_weight)  # 10 gives a better spread\n",
    "\n",
    "        # return the adjacency matrix of the graph for lrp purposes\n",
    "        A = to_dense_adj(edge_index.to('cpu'), edge_attr=edge_weight.to('cpu'))[0]  # adjacency matrix\n",
    "\n",
    "        # message passing\n",
    "        out = self.propagate(edge_index, x=(msg_activations, None),\n",
    "                             edge_weight=edge_weight,\n",
    "                             size=(s_l.size(0), s_r.size(0)))\n",
    "\n",
    "        return self.lin_out(out), A, msg_activations\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
    "        return x_j * edge_weight.unsqueeze(1)\n",
    "\n",
    "    def aggregate(self, inputs: Tensor, index: Tensor,\n",
    "                  dim_size: Optional[int] = None) -> Tensor:\n",
    "        out_mean = scatter(inputs, index, dim=self.node_dim, dim_size=dim_size,\n",
    "                           reduce='sum')\n",
    "        return out_mean\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.in_channels}, '\n",
    "                f'{self.out_channels}, k={self.k})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from X (the input to the tensorflow model, reshape and recast as conveninet pytorch format)\n",
    "pytorch_X = torch.tensor(X[:1].reshape(-1,12)) # the slice [:1] picks up the first event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple FCN and perform forward pass\n",
    "model = MLPF_FCN()\n",
    "model(pytorch_X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a GNN and perform forward pass\n",
    "batch = Batch(x = pytorch_X) # recall GNN takes a batch object\n",
    "model = MLPF_GNN()\n",
    "model(batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines lrp\n",
    "\n",
    "import pickle as pkl\n",
    "import os.path as osp\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "\n",
    "\n",
    "class LRP_MLPF():\n",
    "\n",
    "    \"\"\"\n",
    "    Extends the LRP class to act on graph datasets and GNNs based on the Gravnet layer (e.g. the MLPF model, see models.MLPF)\n",
    "    The main trick is to realize that the \".lin_s\" layers in Gravnet are irrelevant for explanations so shall be skipped\n",
    "    The hack, however, is to substitute them precisely with the message_passing step\n",
    "\n",
    "    Differences from standard LRP\n",
    "        a. Rscores become tensors/graphs of input features per output neuron instead of vectors\n",
    "        b. accomodates message passing steps by using the adjacency matrix as the weight matrix in standard LRP,\n",
    "           and redistributing Rscores over the other dimension (over nodes instead of features)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device, model, epsilon):\n",
    "\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.epsilon = epsilon  # for stability reasons in the lrp-epsilon rule (by default: a very small number)\n",
    "\n",
    "        # check if the model has any skip connections to accomodate them\n",
    "        self.skip_connections = self.find_skip_connections()\n",
    "        self.msg_passing_layers = self.find_msg_passing_layers()\n",
    "\n",
    "    \"\"\"\n",
    "    explanation functions\n",
    "    \"\"\"\n",
    "\n",
    "    def explain(self, input, neuron_to_explain):\n",
    "        \"\"\"\n",
    "        Primary function to call on an LRP instance to start explaining predictions.\n",
    "        First, it registers hooks and runs a forward pass on the input.\n",
    "        Then, it attempts to explain the whole model by looping over the layers in the model and invoking the explain_single_layer function.\n",
    "\n",
    "        Args:\n",
    "            input: tensor containing the input sample you wish to explain\n",
    "            neuron_to_explain: the index for a particular neuron in the output layer you wish to explain\n",
    "\n",
    "        Returns:\n",
    "            R_tensor: a tensor/graph containing the relevance scores of the input graph for a particular output neuron\n",
    "            preds: the model predictions of the input (for further plotting/processing purposes only)\n",
    "            input: the input that was explained (for further plotting/processing purposes only)\n",
    "        \"\"\"\n",
    "\n",
    "        # register forward hooks to retrieve intermediate activations\n",
    "        # in simple words, when the forward pass is called, the following dict() will be filled with (key, value) = (\"layer_name\", activations)\n",
    "        activations = {}\n",
    "\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activations[name] = input[0]\n",
    "            return hook\n",
    "\n",
    "        for name, module in self.model.named_modules():\n",
    "            # unfold any containers so as to register hooks only for their child modules (equivalently we are demanding type(module) != nn.Sequential))\n",
    "            if ('Linear' in str(type(module))) or ('activation' in str(type(module))) or ('BatchNorm1d' in str(type(module))):\n",
    "                module.register_forward_hook(get_activation(name))\n",
    "\n",
    "        # run a forward pass\n",
    "        self.model.eval()\n",
    "        preds, self.A, self.msg_activations = self.model(input.to(self.device))\n",
    "\n",
    "        # get the activations\n",
    "        self.activations = activations\n",
    "        self.num_layers = len(activations.keys())\n",
    "        self.in_features_dim = self.name2layer(list(activations.keys())[0]).in_features\n",
    "\n",
    "        print(f'Total number of layers: {self.num_layers}')\n",
    "\n",
    "        # initialize Rscores for skip connections (in case there are any)\n",
    "        if len(self.skip_connections) != 0:\n",
    "            self.skip_connections_relevance = 0\n",
    "\n",
    "        # initialize the Rscores tensor using the output predictions\n",
    "        Rscores = preds[:, neuron_to_explain].reshape(-1, 1).detach()\n",
    "\n",
    "        # build the Rtensor which is going to be a whole graph of Rscores per node\n",
    "        R_tensor = torch.zeros([Rscores.shape[0], Rscores.shape[0], Rscores.shape[1]]).to(self.device)\n",
    "        for node in range(R_tensor.shape[0]):\n",
    "            R_tensor[node][node] = Rscores[node]\n",
    "\n",
    "        # loop over layers in the model to propagate Rscores backward\n",
    "        for layer_index in range(self.num_layers, 0, -1):\n",
    "            R_tensor = self.explain_single_layer(R_tensor, layer_index, neuron_to_explain)\n",
    "\n",
    "        print(\"Finished explaining all layers.\")\n",
    "\n",
    "        if len(self.skip_connections) != 0:\n",
    "            return R_tensor + self.skip_connections_relevance, preds, input\n",
    "\n",
    "        return R_tensor, preds, input\n",
    "\n",
    "    def explain_single_layer(self, R_tensor_old, layer_index, neuron_to_explain):\n",
    "        \"\"\"\n",
    "        Attempts to explain a single layer in the model by propagating Rscores backwards using the lrp-epsilon rule.\n",
    "\n",
    "        Args:\n",
    "            R_tensor_old: a tensor/graph containing the Rscores, of the current layer, to be propagated backwards\n",
    "            layer_index: index that corresponds to the position of the layer in the model (see helper functions)\n",
    "            neuron_to_explain: the index for a particular neuron in the output layer to explain\n",
    "\n",
    "        Returns:\n",
    "            R_tensor_new: a tensor/graph containing the computed Rscores of the previous layer\n",
    "        \"\"\"\n",
    "\n",
    "        # get layer information\n",
    "        layer_name = self.index2name(layer_index)\n",
    "        layer = self.name2layer(layer_name)\n",
    "\n",
    "        # get layer activations (depends wether it's a message passing step)\n",
    "        if layer_name in self.msg_passing_layers.keys():\n",
    "            print(f\"Explaining layer {self.num_layers+1-layer_index}/{self.num_layers}: MessagePassing layer\")\n",
    "            input = self.msg_activations[layer_name[:-6]].to(self.device).detach()\n",
    "            msg_passing_layer = True\n",
    "        else:\n",
    "            print(f\"Explaining layer {self.num_layers+1-layer_index}/{self.num_layers}: {layer}\")\n",
    "            input = self.activations[layer_name].to(self.device).detach()\n",
    "            msg_passing_layer = False\n",
    "\n",
    "        # run lrp\n",
    "        if 'Linear' in str(layer):\n",
    "            R_tensor_new = self.eps_rule(self, layer, layer_name, input, R_tensor_old, neuron_to_explain, msg_passing_layer)\n",
    "            print('- Finished computing Rscores')\n",
    "            return R_tensor_new\n",
    "        else:\n",
    "            if 'activation' in str(layer):\n",
    "                print(f\"- skipping layer because it's an activation layer\")\n",
    "            elif 'BatchNorm1d' in str(layer):\n",
    "                print(f\"- skipping layer because it's a BatchNorm layer\")\n",
    "            print(f\"- Rscores do not need to be computed\")\n",
    "            return R_tensor_old\n",
    "\n",
    "    \"\"\"\n",
    "    lrp-epsilon rule\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def eps_rule(self, layer, layer_name, x, R_tensor_old, neuron_to_explain, msg_passing_layer):\n",
    "        \"\"\"\n",
    "        Implements the lrp-epsilon rule presented in the following reference: https://doi.org/10.1007/978-3-030-28954-6_10.\n",
    "\n",
    "        Can accomodate message_passing layers if the adjacency matrix and the activations before the message_passing are provided.\n",
    "        The trick (or as we like to call it, the message_passing hack) is in\n",
    "            a. using the adjacency matrix as the weight matrix in the standard lrp rule\n",
    "            b. transposing the activations to distribute the Rscores over the other dimension (over nodes instead of features)\n",
    "\n",
    "        Args:\n",
    "            layer: a torch.nn module with a corresponding weight matrix W\n",
    "            x: vector containing the activations of the previous layer\n",
    "            R_tensor_old: a tensor/graph containing the Rscores, of the current layer, to be propagated backwards\n",
    "            neuron_to_explain: the index for a particular neuron in the output layer to explain\n",
    "\n",
    "        Returns:\n",
    "            R_tensor_new: a tensor/graph containing the computed Rscores of the previous layer\n",
    "        \"\"\"\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if msg_passing_layer:   # message_passing hack\n",
    "            x = torch.transpose(x, 0, 1)               # transpose the activations to distribute the Rscores over the other dimension (over nodes instead of features)\n",
    "            W = self.A[layer_name[:-6]].detach().to(self.device)       # use the adjacency matrix as the weight matrix\n",
    "        else:\n",
    "            W = layer.weight.detach()  # get weight matrix\n",
    "            W = torch.transpose(W, 0, 1)    # sanity check of forward pass: (torch.matmul(x, W) + layer.bias) == layer(x)\n",
    "\n",
    "        # for the output layer, pick the part of the weight matrix connecting only to the neuron you're attempting to explain\n",
    "        if layer == list(self.model.modules())[-1]:\n",
    "            W = W[:, neuron_to_explain].reshape(-1, 1)\n",
    "\n",
    "        # (1) compute the denominator\n",
    "        denominator = torch.matmul(x, W) + self.epsilon\n",
    "        # (2) scale the Rscores\n",
    "        if msg_passing_layer:  # message_passing hack\n",
    "            R_tensor_old = torch.transpose(R_tensor_old, 1, 2)\n",
    "        scaledR = R_tensor_old / denominator\n",
    "        # (3) compute the new Rscores\n",
    "        R_tensor_new = torch.matmul(scaledR, torch.transpose(W, 0, 1)) * x\n",
    "\n",
    "        # checking conservation of Rscores for a given random node (# 17)\n",
    "        rtol = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "        for tol in rtol:\n",
    "            if (torch.allclose(R_tensor_new[17].sum(), R_tensor_old[17].sum(), rtol=tol)):\n",
    "                print(f'- Rscores are conserved up to relative tolerance {str(tol)}')\n",
    "                break\n",
    "\n",
    "        if layer in self.skip_connections:\n",
    "            # set aside the relevance of the input_features in the skip connection\n",
    "            # recall: it is assumed that the skip connections are defined in the following order torch.cat[(input_features, ...)] )\n",
    "            self.skip_connections_relevance = self.skip_connections_relevance + R_tensor_new[:, :, :self.in_features_dim]\n",
    "            return R_tensor_new[:, :, self.in_features_dim:]\n",
    "\n",
    "        if msg_passing_layer:  # message_passing hack\n",
    "            return torch.transpose(R_tensor_new, 1, 2)\n",
    "\n",
    "        return R_tensor_new\n",
    "\n",
    "    \"\"\"\n",
    "    helper functions\n",
    "    \"\"\"\n",
    "\n",
    "    def index2name(self, layer_index):\n",
    "        \"\"\"\n",
    "        Given the index of a layer (e.g. 3) returns the name of the layer (e.g. .nn1.3)\n",
    "        \"\"\"\n",
    "        layer_name = list(self.activations.keys())[layer_index - 1]\n",
    "        return layer_name\n",
    "\n",
    "    def name2layer(self, layer_name):\n",
    "        \"\"\"\n",
    "        Given the name of a layer (e.g. .nn1.3) returns the corresponding torch module (e.g. Linear(...))\n",
    "        \"\"\"\n",
    "        for name, module in self.model.named_modules():\n",
    "            if layer_name == name:\n",
    "                return module\n",
    "\n",
    "    def find_skip_connections(self):\n",
    "        \"\"\"\n",
    "        Given a torch model, retuns a list of layers with skip connections... the elements are torch modules (e.g. Linear(...))\n",
    "        \"\"\"\n",
    "        explainable_layers = []\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'lin_s' in name:     # for models that are based on Gravnet, skip the lin_s layers\n",
    "                continue\n",
    "            if ('Linear' in str(type(module))):\n",
    "                explainable_layers.append(module)\n",
    "\n",
    "        skip_connections = []\n",
    "        for layer_index in range(len(explainable_layers) - 1):\n",
    "            if explainable_layers[layer_index].out_features != explainable_layers[layer_index + 1].in_features:\n",
    "                skip_connections.append(explainable_layers[layer_index + 1])\n",
    "\n",
    "        return skip_connections\n",
    "\n",
    "    def find_msg_passing_layers(self):\n",
    "        \"\"\"\n",
    "        Returns a list of \".lin_s\" layers from model.named_modules() that shall be substituted with message passing\n",
    "        \"\"\"\n",
    "        msg_passing_layers = {}\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'lin_s' in name:     # for models that are based on Gravnet, replace the .lin_s layers with message_passing\n",
    "                msg_passing_layers[name] = {}\n",
    "\n",
    "        return msg_passing_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers: 13\n",
      "Explaining layer 1/13: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Finished computing Rscores\n",
      "Explaining layer 2/13: ReLU()\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 3/13: Linear(in_features=14, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 4/13: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 5/13: ReLU()\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 6/13: BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- skipping layer because it's a BatchNorm layer\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 7/13: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 8/13: ReLU()\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 9/13: BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- skipping layer because it's a BatchNorm layer\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 10/13: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 11/13: ReLU()\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 12/13: BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "- skipping layer because it's a BatchNorm layer\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 13/13: Linear(in_features=12, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Finished explaining all layers.\n"
     ]
    }
   ],
   "source": [
    "# test lrp for the FCN model\n",
    "model = MLPF_FCN()\n",
    "\n",
    "lrp_instance = LRP_MLPF('cpu', model, epsilon=1e-9)\n",
    "Rtensor, pred, inputt = lrp_instance.explain(pytorch_X, neuron_to_explain=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers: 27\n",
      "Explaining layer 1/27: Linear(in_features=2, out_features=6, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 2/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 3/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 4/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 5/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 6/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 7/27: Linear(in_features=18, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 8/27: Linear(in_features=2, out_features=6, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 9/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 10/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 11/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 12/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 13/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 14/27: Linear(in_features=14, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 15/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 16/27: MessagePassing layer\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 17/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 18/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 19/27: MessagePassing layer\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 20/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 21/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 22/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 23/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 24/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 25/27: Linear(in_features=2, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Explaining layer 26/27: ELU(alpha=1.0)\n",
      "- Rscores do not need to be computed\n",
      "Explaining layer 27/27: Linear(in_features=12, out_features=2, bias=True)\n",
      "- Rscores are conserved up to relative tolerance 1e-05\n",
      "- Finished computing Rscores\n",
      "Finished explaining all layers.\n"
     ]
    }
   ],
   "source": [
    "# test lrp for the GNN model\n",
    "model = MLPF_GNN()\n",
    "\n",
    "lrp_instance = LRP_MLPF('cpu', model, epsilon=1e-9)\n",
    "Rtensor, pred, inputt = lrp_instance.explain(batch, neuron_to_explain=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the tensorflow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the first event\n",
    "input_classes = np.unique(X[:, :, 0].flatten())\n",
    "output_classes = np.unique(y[:, :, 0].flatten())\n",
    "num_output_classes = len(output_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(y):\n",
    "    return {\n",
    "        \"cls\": tf.one_hot(tf.cast(y[:, :, 0], tf.int32), num_output_classes),\n",
    "        \"charge\": y[:, :, 1:2],\n",
    "        \"pt\": y[:, :, 2:3],\n",
    "        \"eta\": y[:, :, 3:4],\n",
    "        \"sin_phi\": y[:, :, 4:5],\n",
    "        \"cos_phi\": y[:, :, 5:6],\n",
    "        \"energy\": y[:, :, 6:7],\n",
    "    }\n",
    "yt = transform_target(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfmodel.model import PFNetDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_true_particle = y[:, :, 0]!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y[msk_true_particle][:, 0], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yt[\"pt\"][msk_true_particle].flatten(), bins=100);\n",
    "plt.xlabel(\"pt\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yt[\"eta\"][msk_true_particle].flatten(), bins=100);\n",
    "plt.xlabel(\"eta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yt[\"sin_phi\"][msk_true_particle].flatten(), bins=100);\n",
    "plt.xlabel(\"sin phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yt[\"cos_phi\"][msk_true_particle].flatten(), bins=100);\n",
    "plt.xlabel(\"cos phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(yt[\"energy\"][msk_true_particle].flatten(), bins=100);\n",
    "plt.xlabel(\"energy\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PFNetDense(\n",
    "    num_input_classes=len(input_classes),\n",
    "    num_output_classes=len(output_classes),\n",
    "    activation=tf.nn.elu,\n",
    "    hidden_dim=128,\n",
    "    bin_size=128,\n",
    "    input_encoding=\"default\",\n",
    "    multi_output=True\n",
    ")\n",
    "\n",
    "# #temporal weight mode means each input element in the event can get a separate weight\n",
    "model.compile(\n",
    "    loss={\n",
    "        \"cls\": tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        \"charge\": tf.keras.losses.MeanSquaredError(),\n",
    "        \"pt\": tf.keras.losses.MeanSquaredError(),\n",
    "        \"energy\": tf.keras.losses.MeanSquaredError(),\n",
    "        \"eta\": tf.keras.losses.MeanSquaredError(),\n",
    "        \"sin_phi\": tf.keras.losses.MeanSquaredError(),\n",
    "        \"cos_phi\": tf.keras.losses.MeanSquaredError()\n",
    "    },\n",
    "    optimizer=\"adam\",\n",
    "    sample_weight_mode=\"temporal\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, yt, epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = model.predict(X, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index of the class prediction output values\n",
    "pred_id_offset = len(output_classes)\n",
    "ypred_ids_raw = ypred[\"cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(\n",
    "    np.argmax(ypred_ids_raw, axis=-1).flatten(),\n",
    "    np.argmax(yt[\"cls\"], axis=-1).flatten(), labels=output_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_particles = (X[:, :, 0]!=0)\n",
    "plt.scatter(\n",
    "    ypred[\"eta\"][msk_particles].flatten(),\n",
    "    yt[\"eta\"][msk_particles].flatten(), marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
