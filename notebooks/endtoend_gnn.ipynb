{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import EdgeConv, MessagePassing, EdgePooling\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.data import Dataset, Data, DataLoader\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PFGraphDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, connect_all=False, max_elements=None, max_candidates=None):\n",
    "        self._connect_all = connect_all\n",
    "        self._max_elements = max_elements\n",
    "        self._max_candidates = max_candidates\n",
    "        super(PFGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.raw_dir = root\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        raw_list = glob(self.raw_dir+'/*ev*.npz')\n",
    "        return sorted([l.replace(self.raw_dir,'.') for l in raw_list])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_{}.pt'.format(i) for i in range(len(self.raw_file_names))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        feature_scale = np.array([1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "        i = 0\n",
    "        \n",
    "        for raw_file_name in self.raw_file_names:\n",
    "            \n",
    "            dist_file_name = raw_file_name.replace('ev','dist')\n",
    "            print(\"loading data from files: {0}, {1}\".format(osp.join(self.raw_dir, raw_file_name), osp.join(self.raw_dir, dist_file_name)))\n",
    "            try:\n",
    "                fi = np.load(osp.join(self.raw_dir, raw_file_name))\n",
    "                fi_dist = np.load(osp.join(self.raw_dir, dist_file_name))\n",
    "            except Exception as e:\n",
    "                print(\"Could not open files: {0}, {1}\".format(osp.join(self.raw_dir, raw_file_name), osp.join(self.raw_dir, dist_file_name)))\n",
    "                continue\n",
    "            \n",
    "            X_elements = fi['elements'][:self._max_elements]\n",
    "            X_element_block_id = fi['element_block_id'][:self._max_elements]\n",
    "            y_candidates = fi['candidates'][:self._max_candidates, 1:]\n",
    "            y_candidate_block_id = fi['candidate_block_id'][:self._max_candidates]\n",
    "            num_elements = X_elements.shape[0]\n",
    "\n",
    "            row_index = fi_dist['row']\n",
    "            col_index = fi_dist['col']\n",
    "            num_edges = row_index.shape[0]\n",
    "\n",
    "            edge_index = np.zeros((2, 2*num_edges))\n",
    "            edge_index[0,:num_edges] = row_index\n",
    "            edge_index[1,:num_edges] = col_index\n",
    "            edge_index[0,num_edges:] = col_index\n",
    "            edge_index[1,num_edges:] = row_index\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "            edge_data = fi_dist['data']\n",
    "            edge_attr = np.zeros((2*num_edges,1))\n",
    "            edge_attr[:num_edges,0] = edge_data\n",
    "            edge_attr[num_edges:,0] = edge_data\n",
    "            edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "            x = torch.tensor(X_elements/feature_scale, dtype=torch.float)\n",
    "\n",
    "            #y = [X_element_block_id[i]==X_element_block_id[j] for (i,j) in edge_index.t().contiguous()]\n",
    "            y = torch.tensor(y_candidates, dtype=torch.float)\n",
    "\n",
    "            data = Data(x=x, edge_index=edge_index, y=y, edge_attr=edge_attr)\n",
    "            data.x_cluster_labels = torch.tensor(X_element_block_id, dtype=torch.float)\n",
    "            data.y_cluster_labels = torch.tensor(y_candidate_block_id, dtype=torch.float)\n",
    "            \n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(i)))\n",
    "            i += 1\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(idx)))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = PFGraphDataset(root='../data/TTbar_run3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../data/TTBar_run3/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_dataset.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_dataset.get(0)\n",
    "input_dim = data.x.shape[1]\n",
    "edge_dim = data.edge_attr.shape[1]\n",
    "fulllen = len(full_dataset)\n",
    "\n",
    "tv_frac = 0.10\n",
    "tv_num = math.ceil(fulllen*tv_frac)\n",
    "splits = np.cumsum([fulllen-2*tv_num,tv_num,tv_num])\n",
    "\n",
    "batch_size = 64\n",
    "n_epochs = 100\n",
    "lr = 0.001\n",
    "patience = 10\n",
    "hidden_dim = 32\n",
    "n_iters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConvWithEdgeAttr(MessagePassing):\n",
    "    def __init__(self, nn, aggr='max', **kwargs):\n",
    "        super(EdgeConvWithEdgeAttr, self).__init__(aggr=aggr, **kwargs)\n",
    "        self.nn = nn\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\"\"\"\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "        pseudo = edge_attr.unsqueeze(-1) if edge_attr.dim() == 1 else edge_attr\n",
    "        return self.propagate(edge_index, x=x, pseudo=pseudo)\n",
    "\n",
    "    def message(self, x_i, x_j, pseudo):\n",
    "        return self.nn(torch.cat([x_i, x_j - x_i, pseudo], dim=1))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(nn={})'.format(self.__class__.__name__, self.nn)\n",
    "\n",
    "class EdgeNet(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, edge_dim=1, output_dim=1, n_iters=1, aggr='add'):\n",
    "        super(EdgeNet, self).__init__()\n",
    "        \n",
    "        convnn = nn.Sequential(nn.Linear(2*(hidden_dim + input_dim)+edge_dim, 2*hidden_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(2*hidden_dim, hidden_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(hidden_dim, hidden_dim),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(hidden_dim, hidden_dim),\n",
    "                               nn.ReLU()\n",
    "        )\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "        self.batchnorm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(40)\n",
    "\n",
    "        self.inputnet =  nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "#         self.edgenetwork = nn.Sequential(nn.Linear(2*(hidden_dim+input_dim)+edge_dim,2*hidden_dim),\n",
    "#                                          nn.ReLU(),\n",
    "#                                          nn.Linear(2*hidden_dim, output_dim),\n",
    "#                                          nn.Sigmoid())\n",
    "\n",
    "        self.nodenetwork = EdgeConvWithEdgeAttr(nn=convnn, aggr=aggr)\n",
    "        \n",
    "        self.pooling1 = EdgePooling(40, dropout=0.2)\n",
    "        self.pooling2 = EdgePooling(40, dropout=0.2)\n",
    "        self.pooling3 = EdgePooling(40, dropout=0.2)\n",
    "        \n",
    "        self.outnetwork = nn.Sequential(nn.Linear(40, 100),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(100, 100),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(100, 100),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(100, 100),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Linear(100, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):        \n",
    "        X = self.batchnorm1(data.x)\n",
    "        H = self.inputnet(data.x)\n",
    "        x = torch.cat([H,X],dim=-1)\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            H = self.nodenetwork(x, data.edge_index, data.edge_attr)\n",
    "            x = torch.cat([H,X],dim=-1)\n",
    "\n",
    "        #row,col = data.edge_index        \n",
    "        #output = self.edgenetwork(torch.cat([x[row], x[col], data.edge_attr],dim=-1)).squeeze(-1)\n",
    "\n",
    "        pooled, edge_index, batch, unpool_info1 = self.pooling1(x, data.edge_index, data.batch)\n",
    "        pooled, edge_index, batch, unpool_info2 = self.pooling2(pooled, edge_index, batch)\n",
    "        pooled, edge_index, batch, unpool_info3 = self.pooling3(pooled, edge_index, batch)\n",
    "        #pooled: (N, 40)\n",
    "        \n",
    "        r = self.outnetwork(self.batchnorm2(pooled))\n",
    "        \n",
    "        #print(unpool_info)\n",
    "        #print(x.shape, pooled.shape, r.shape, unpool_info.cluster.shape)\n",
    "        \n",
    "        return r, unpool_info1.cluster, unpool_info2.cluster, unpool_info3.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_vs_pred(data, prd):\n",
    "    preds = []\n",
    "    tgts = []\n",
    "    n_tgts = []\n",
    "    n_preds = []\n",
    "    \n",
    "    for cl in np.unique(data.y_cluster_labels):\n",
    "        m1 = torch.tensor(data.y_cluster_labels == cl)\n",
    "        m2 = torch.tensor(data.x_cluster_labels == cl)\n",
    "        pred = prd[0][prd[3][prd[2][prd[1][m2]]]]\n",
    "        tgt = data.y[m1]\n",
    "        n_preds += [pred.shape[0]]\n",
    "        n_tgts += [tgt.shape[0]]\n",
    "        n = min(pred.shape[0], tgt.shape[0])\n",
    "        preds += [pred.detach().numpy()[:n]]\n",
    "        tgts += [tgt.detach().numpy()[:n]]\n",
    "\n",
    "    prd = model(data)\n",
    "    tgts = np.concatenate(tgts)\n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds, tgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(full_dataset, batch_size=1, pin_memory=True, shuffle=False)\n",
    "model = EdgeNet(input_dim=input_dim, hidden_dim=hidden_dim, edge_dim=edge_dim, n_iters=n_iters).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "print(model)\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params)\n",
    "\n",
    "model.train()\n",
    "data = data.to(device)\n",
    "\n",
    "losses = []\n",
    "corrs = []\n",
    "for j in range(200):\n",
    "    losses_batch = []\n",
    "    corrs_batch = []\n",
    "    \n",
    "    num_pred = []\n",
    "    num_true = []\n",
    "    for i in range(20):\n",
    "        data = full_dataset.get(i)\n",
    "        data.batch = torch.zeros(data.x.shape[0], dtype=torch.long)\n",
    "        optimizer.zero_grad()\n",
    "        batch_target = data.y        \n",
    "        batch_output, pool_clusters1, pool_clusters2, pool_clusters3 = model(data)\n",
    "        \n",
    "        num_pred += [batch_output.shape[0]]\n",
    "        num_true += [data.y.shape[0]]\n",
    "\n",
    "        #Compute loss within each block that are known in advance\n",
    "        batch_loss = []\n",
    "        \n",
    "        #loop over all block ids\n",
    "        for cl in np.unique(data.y_cluster_labels):\n",
    "            \n",
    "            #find the elements and candidates corresponding to this block\n",
    "            m1 = torch.tensor(data.y_cluster_labels == cl)\n",
    "            m2 = torch.tensor(data.x_cluster_labels == cl)\n",
    "            \n",
    "            #get the predicted and target candidates that use elements from this block\n",
    "            pred = batch_output[pool_clusters3[pool_clusters2[pool_clusters1[m2]]]]\n",
    "            tgt = data.y[m1]\n",
    "                        \n",
    "            n = min(pred.shape[0], tgt.shape[0])\n",
    "            batch_loss += [\n",
    "                loss(pred[:n], tgt[:n]) + (pred[n:]**2).sum() + (tgt[n:]**2).sum()\n",
    "            ]\n",
    "        #print(batch_output.shape[0], data.y.shape[0])\n",
    "        \n",
    "        batch_loss = sum(batch_loss)\n",
    "        batch_loss.backward()\n",
    "        batch_loss_item = batch_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        tgts, preds = get_target_vs_pred(data, (batch_output, pool_clusters1, pool_clusters2, pool_clusters3))\n",
    "        corr_pt = np.corrcoef(tgts[:, 0], preds[:, 0])[0,1]\n",
    "        corrs_batch += [corr_pt]\n",
    "        losses_batch += [batch_loss_item]\n",
    "    \n",
    "    l = np.mean(losses_batch)\n",
    "    losses += [l]\n",
    "    corrs += [np.mean(corrs_batch)]\n",
    "    print(j, l, corrs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full_dataset.get(0)\n",
    "data.batch = torch.zeros(data.x.shape[0], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(tgts[:, 0], preds[:, 0], marker=\".\", alpha=0.5)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.plot([-5,5],[-5,5], lw=1, color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(0,10,100)\n",
    "plt.hist(data.y[:, 0].detach().numpy(), bins=b, lw=2, histtype=\"step\");\n",
    "plt.hist(prd[0][:, 0].detach().numpy(), bins=b, lw=2, histtype=\"step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(-5,5,40)\n",
    "plt.hist(data.y[:, 1].detach().numpy(), bins=b, lw=2, histtype=\"step\");\n",
    "plt.hist(prd[0][:, 1].detach().numpy(), bins=b, lw=2, histtype=\"step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(-5, 5, 40)\n",
    "plt.hist(data.y[:, 2].detach().numpy(), bins=b, lw=2, histtype=\"step\");\n",
    "plt.hist(prd[0][:, 2].detach().numpy(), bins=b, lw=2, histtype=\"step\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
