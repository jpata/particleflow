backend: tensorflow

dataset:
  schema: cms
  target_particles: cand
  num_input_features: 18
  num_output_features: 7
#       NONE = 0,
#       TRACK = 1,
#       PS1 = 2,
#       PS2 = 3,
#       ECAL = 4,
#       HCAL = 5,
#       GSF = 6,
#       BREM = 7,
#       HFEM = 8,
#       HFHAD = 9,
#       SC = 10,
#       HO = 11,
  num_input_classes: 12
  #(none=0, ch.had=1, n.had=2, hfem=3, hfhad=4, gamma=5, e=6, mu=7)
  num_output_classes: 8
  padded_num_elem_size: 6400
  #(pt, eta, sin phi, cos phi, E)
  num_momentum_outputs: 5
  classification_loss_coef: 100.0
  charge_loss_coef: 0.01
  pt_loss_coef: 0.0001
  eta_loss_coef: 100.0
  sin_phi_loss_coef: 10.0
  cos_phi_loss_coef: 10.0
  energy_loss_coef: 0.0001
  energy_loss:
    type: Huber
  pt_loss:
    type: Huber
  sin_phi_loss:
    type: Huber
    delta: 0.1
  cos_phi_loss:
    type: Huber
    delta: 0.1
  eta_loss:
    type: Huber
    delta: 0.1

tensorflow:
  eager: no

setup:
  train: yes
  weights:
  weights_config:
  lr: 1e-4
  num_events_validation: 100
  num_epochs: 1000
  dtype: float32
  trainable:
  classification_loss_type: sigmoid_focal_crossentropy
  lr_schedule: exponentialdecay  # exponentialdecay, onecycle
  optimizer: adam  # adam, adamw, sgd

optimizer:
  adam:
    amsgrad: no
  adamw:
    amsgrad: yes
    weight_decay: 0.001
  sgd:
    nesterov: no
    momentum: 0.9

# LR Schedules
exponentialdecay:
  decay_steps: 10000
  decay_rate: 0.99
  staircase: yes
onecycle:
  mom_min: 0.85
  mom_max: 0.95
  warmup_ratio: 0.3
  div_factor: 25.0
  final_div: 100000.0

parameters:
  model: gnn_dense
  input_encoding: cms
  node_update_mode: concat
  do_node_encoding: no
  node_encoding_hidden_dim: 128
  combined_graph_layer:
    bin_size: 160
    max_num_bins: 200
    distance_dim: 64
    layernorm: no
    dropout: 0.0
    dist_activation: elu
    ffn_dist_num_layers: 1
    ffn_dist_hidden_dim: 128
    kernel:
      type: NodePairGaussianKernel
      dist_mult: 0.1
      clip_value_low: 0.0
    #kernel:
    #  type: NodePairTrainableKernel
    #  output_dim: 4
    #  hidden_dim_pair: 32
    #  hidden_dim_node: 128
    #  num_layers: 1
    num_node_messages: 1
    node_message:
      type: GHConvDense
      output_dim: 256
      activation: elu
      normalize_degrees: yes
    #node_message:
    #  type: NodeMessageLearnable
    #  output_dim: 128
    #  hidden_dim: 128
    #  num_layers: 1
    #  activation: gelu
    activation: elu
  num_graph_layers_common: 2
  num_graph_layers_energy: 2
  output_decoding:
    activation: elu
    regression_use_classification: yes
    dropout: 0.0

    pt_skip_gate: no
    eta_skip_gate: yes
    phi_skip_gate: yes

    id_dim_decrease: yes
    charge_dim_decrease: yes
    pt_dim_decrease: yes
    eta_dim_decrease: yes
    phi_dim_decrease: yes
    energy_dim_decrease: yes

    id_hidden_dim: 512
    charge_hidden_dim: 256
    pt_hidden_dim: 256
    eta_hidden_dim: 256
    phi_hidden_dim: 256
    energy_hidden_dim: 256

    id_num_layers: 3
    charge_num_layers: 2
    pt_num_layers: 2
    eta_num_layers: 2
    phi_num_layers: 2
    energy_num_layers: 3
    layernorm: yes
    mask_reg_cls0: no

    energy_multimodal: no

  skip_connection: yes
  debug: no

timing:
  num_ev: 100
  num_iter: 3

callbacks:
  checkpoint:
    save_weights_only: yes
    monitor: "val_loss"
    save_best_only: no
  plot_freq: 10
  tensorboard:
    dump_history: yes
    hist_freq: 1

hypertune:
  algorithm: hyperband  # random, bayesian, hyperband
  random:
    objective: val_loss
    max_trials: 100
  bayesian:
    objective: val_loss
    max_trials: 100
    num_initial_points: 2
  hyperband:
    objective: val_loss
    max_epochs: 10
    factor: 3
    iterations: 1
    executions_per_trial: 1

raytune:
  local_dir:  # Note: please specify an absolute path
  sched: "asha"  # asha, hyperband
  parameters:
    # optimizer parameters
    lr: [1e-4]
    batch_size: [32]
    expdecay_decay_steps: [10000]
    # model parameters
    combined_graph_layer:
      layernorm: [False]
      hidden_dim: [64, 128, 256]
      distance_dim: [128, 256]
      num_node_messages: [1]
      node_message:
        normalize_degrees: [True]
        output_dim: [64, 128, 256]
      dropout: [0.0]
      bin_size: [80, 160, 320]
      kernel:
        clip_value_low: [0.0]
    num_graph_layers_common: [2, 3, 4]
    num_graph_layers_energy: [2, 3, 4]
  # Tune schedule specific parameters
  asha:
    max_t: 100
    reduction_factor: 3
    brackets: 1
    grace_period: 5
  hyperband:
    max_t: 100
    reduction_factor: 3

training_datasets:
  - cms_pf_ttbar
  - cms_pf_single_electron
  - cms_pf_single_pi
  - cms_pf_single_tau

testing_datasets:
  - cms_pf_ttbar
  - cms_pf_single_electron
  - cms_pf_single_pi
  - cms_pf_single_tau

validation_dataset: cms_pf_ttbar

datasets: 
  cms_pf_ttbar:
    version: 1.1.0
    data_dir:
    manual_dir:
    batch_per_gpu: 5
  cms_pf_single_pi:
    version: 1.1.0
    data_dir:
    manual_dir:
    batch_per_gpu: 50
  cms_pf_single_tau:
    version: 1.1.0
    data_dir:
    manual_dir:
    batch_per_gpu: 50
  cms_pf_single_electron:
    version: 1.1.0
    data_dir:
    manual_dir:
    batch_per_gpu: 50
